% LLM_course_slides.tex
% Cours de Traitement du Langage Naturel (NLP)

\documentclass[aspectratio=169]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xcolor}
\definecolor{lightblue}{rgb}{0.68,0.85,0.9}
\definecolor{lightgreen}{rgb}{0.56,0.93,0.56}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{matrix,calc,math}
\usepackage{booktabs}

% Thème Beamer
\usetheme{Madrid}
\usecolortheme{default}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

% Informations du titre
\title{Traitement du Langage Naturel (NLP) \\ \large{Des fondamentaux aux modèles de langage avancés}}
\author{Florian Valade}
\institute{Université Gustave Eiffel}
\date{\today}

\begin{document}

% Slide de titre
\begin{frame}
    \titlepage
\end{frame}

% Slide 2: Les entreprises d'IA
\begin{frame}{L'écosystème des entreprises d'IA}
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \begin{center}
                \includegraphics[width=0.3\textwidth]{images/openai.png} 
                \\ OpenAI (ChatGPT, GPT-4)
                        
                \vspace{0.5cm}
                \includegraphics[width=0.3\textwidth]{images/anthropic.png}
                \\ Anthropic (Claude)
            \end{center}
        \end{column}
        \begin{column}{0.3\textwidth}
            \begin{center}
                \includegraphics[width=0.3\textwidth]{images/mistral.png} 
                \\ Mistral AI
                
                \vspace{0.5cm}
                
                \includegraphics[width=0.3\textwidth]{images/gemini.png} 
                \\ Google (Gemini)
                
                %\vspace{0.5cm}
                
                %\includegraphics[width=0.2\textwidth]{images/Meta_lockup_positive primary_RGB.png} 
                %\\ Meta (LLaMA)
            \end{center}
        \end{column}
        \begin{column}{0.3\textwidth}
                \includegraphics[width=0.5\textwidth]{images/meta-color.png}
                \\ Meta (LLaMA)
        \end{column}
    \end{columns}
\end{frame}

% Slide 3: Capture d'écran de ChatGPT
\begin{frame}{Interface utilisateur des LLMs: ChatGPT}
    \begin{center}
        \includegraphics[width=0.6\textwidth]{images/chatbot.png} 
    \end{center}
    \begin{itemize}
        \item Interface conversationnelle intuitive
        \item Capacité à comprendre le contexte et les instructions
        \item Génération de réponses cohérentes et informatives
        \item Adaptabilité à différents types de requêtes
    \end{itemize}
\end{frame}

% Slide 4: Architecture de GPT
\begin{frame}{Ce qu'il y a derrière l'interface: Architecture des LLMs}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{center}
                \includegraphics[width=0.5\textwidth]{images/gpt2.png}
            \end{center}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{itemize}
                \item Architecture basée sur les Transformers
                \item Milliards de paramètres entraînés sur d'énormes corpus de texte
                \item Mécanisme d'attention pour capturer les dépendances à longue distance
                \item Apprentissage auto-supervisé (prédiction du token suivant)
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 5: Tokenisation
\begin{frame}{Rappel: La tokenisation}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{images/tiktoken.png}
    \end{center}
    \begin{itemize}
        \item Les modèles de langage ne comprennent pas directement le texte
        \item Le texte est converti en séquences de tokens (unités de base)
        \item Différentes méthodes: par caractère, par mot, par sous-mot (BPE)
        \item Les tokens sont ensuite convertis en IDs numériques pour le modèle
    \end{itemize}
\end{frame}

% Slide 6: Génération de texte
\begin{frame}{Comment fonctionnent les LLMs: Génération de texte}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{images/causal_modeling.png} 
    \end{center}
    \begin{itemize}
        \item Génération auto-régressive: un token à la fois
        \item À chaque étape, le modèle prédit le token le plus probable suivant
        \item Le token prédit est ajouté au contexte pour la prédiction suivante
        \item Processus répété jusqu'à obtenir la réponse complète
    \end{itemize}
\end{frame}

% Slide 7: Titre pour la section Transformers
\begin{frame}{Les Transformers}
    \begin{center}
        \Large{\textbf{Architecture révolutionnaire en NLP}}
        
        \vspace{1cm}
        
        %\includegraphics[width=0.6\textwidth]{images/placeholder_transformer_overview.png} % Placeholder pour un aperçu des Transformers
    \end{center}
\end{frame}

% Slide 8: Architecture détaillée de GPT
\begin{frame}{Architecture détaillée des LLMs: Le modèle GPT}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{center}
                \includegraphics[width=0.7\textwidth]{images/gpt2.png}
                                
            \end{center}
        \end{column}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item \textbf{Embedding}: conversion des tokens en vecteurs denses
                \item \textbf{Positional encoding}: intégration de l'information de position
                \item \textbf{N blocs Transformer}: traitement contextuel profond
                \item \textbf{Layer normalization}: stabilise l'apprentissage
                \item \textbf{Connexions résiduelles}: facilite l'entraînement de réseaux profonds
                \item \textbf{Tête de classification}: prédit le token suivant
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}




% Slide 9: Architecture Encoder-Decoder
\begin{frame}{Architecture originale des Transformers: Encoder-Decoder}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.6\textwidth]{images/encoder_decoder.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Architecture présentée dans \textit{"Attention is All You Need"} (2017)
                \item \textbf{Encoder}: traite l'ensemble de la séquence d'entrée
                \item \textbf{Decoder}: génère la séquence de sortie token par token
                \item Parfaite pour la \textbf{traduction automatique}, où la taille de sortie est inconnue
                \item GPT utilise uniquement la partie \textbf{decoder} (auto-régressive)
                \item BERT utilise uniquement la partie \textbf{encoder} (bi-directionnelle)
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 9.5: Focus sur l'architecture GPT - Entrée
\begin{frame}{Focus sur l'entrée du modèle GPT}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.5\textwidth]{images/gpt2_in.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            Plusieurs étapes :
            \begin{itemize}
                \item Tokenisation : Texte $\rightarrow$ Tokens $[context\_length]$
                \item Embedding : Tokens $\rightarrow$ Vecteurs $[context\_length, embedding\_dim]$
                \item Positional encoding : Vecteurs $\rightarrow$ Vecteurs $[context\_length, embedding\_dim]$
                \item Layer normalization : Vecteurs $\rightarrow$ Vecteurs $[context\_length, embedding\_dim]$
                \item Projections QKV : Vecteurs $\rightarrow$ Vecteurs $[context\_length, 3 \times embedding\_dim]$
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 10: Exemple de traitement d'une phrase
\begin{frame}{Du texte aux vecteurs: Tokenisation}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/generated/tokenization_process.png}
    \end{center}
    \begin{itemize}
        \item \textbf{Tokenisation}: découpage du texte en unités (tokens) que le modèle peut traiter
        \item Différentes méthodes: par caractère, par mot, par sous-mot (BPE)
        \item Les tokens sont ensuite convertis en IDs numériques pour le modèle
    \end{itemize}
\end{frame}

% Slide 11: Embedding
\begin{frame}{Embedding: Conversion des tokens en vecteurs}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/generated/embedding_process.png}
            \end{center}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{itemize}
                \item \textbf{Embedding}: transformation des tokens en vecteurs de nombres réels
                \item Dimension d'embedding typique: 768 à 4096 selon la taille du modèle
                \item Les vecteurs capturent les relations sémantiques entre les tokens
                \item La matrice d'embedding est apprise lors de l'entraînement du modèle
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Positional Encoding: Intégrer l'information de position}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.9\textwidth]{images/position_matrix.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Les Transformers n'ont pas de notion intrinsèque de \textbf{position}
                \item Nécessité d'ajouter cette information explicitement
                \item Encodage sinusoïdal (Vaswani et al.):
                \begin{align*}
                    PE_{(pos,2i)} &= \sin(pos/10000^{2i/d}) \\
                    PE_{(pos,2i+1)} &= \cos(pos/10000^{2i/d})
                \end{align*}
                \item \textbf{Propriétés}:
                \begin{itemize}
                    \item Fréquences différentes pour chaque dimension
                    \item Valeurs bornées entre $[-1,1]$
                    \item Permet de généraliser à des positions non vues
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 12: Layer Normalization
\begin{frame}{Layer Normalization: Stabiliser l'apprentissage}

    \begin{align*}
        \hat{x} &= \frac{x - \mu}{\sigma} \\
        y &= \gamma \cdot \hat{x} + \beta
    \end{align*}
    \begin{itemize}
        \item $\mu$ et $\sigma$ sont les moyennes et écarts-types des vecteurs de token
        \item Normalise chaque vecteur de token indépendamment
        \item Réduit la covariance interne (shift covariate)
        \item Stabilise et accélère l'entraînement des réseaux profonds
        \item Paramètres apprenables $\gamma$ et $\beta$ pour préserver la capacité expressive
    \end{itemize}
\end{frame}

% Slide 13: Mécanisme d'attention
\begin{frame}{Le cœur du Transformer: L'attention}
    \begin{center}
        \begin{tikzpicture}[>=latex, node distance=1.5cm, scale=0.65, transform shape]
            % Title for QKV transformations
            \node[text width=12cm, align=center] at (6,1) {\textbf{Projections linéaires pour Query (Q), Key (K), Value (V)}};
            
            % Inputs
            \foreach \i in {1,...,4}
            {
                \node (Input-\i) at (0,-\i*1.5+0.5) {$x_\i$};
            }

            % Hidden Layer - Q
            \foreach \i in {1,...,3}
            {
                \node[draw, circle, minimum size=1cm] (Hidden-\i) at (3,{-\i*1.5+0.5}) {$f$};
            }
            \node at (3,0) {Q};

            % Inputs
            \foreach \i in {1,...,4}
            {
                \node (Input2-\i) at (6,-\i*1.5+0.5) {$x_\i$};
            }
            % Hidden Layer - K
            \foreach \i in {1,...,3}
            {
                \node[draw, circle, minimum size=1cm] (Hidden2-\i) at (9,{-\i*1.5+0.5}) {$f$};
            }
            \node at (9,0) {K};
            
            % Inputs
            \foreach \i in {1,...,4}
            {
                \node (Input3-\i) at (12,-\i*1.5+0.5) {$x_\i$};
            }
            % Hidden Layer - V
            \foreach \i in {1,...,3}
            {
                \node[draw, circle, minimum size=1cm] (Hidden3-\i) at (15,{-\i*1.5+0.5}) {$f$};
            }
            \node at (15,0) {V};
            
            % Draw connections from inputs to the hidden layer
            \foreach \i in {1,...,4}
            {
                \foreach \j in {1,...,3}
                {
                    \draw[->] (Input-\i) -- (Hidden-\j);
                    \draw[->] (Input2-\i) -- (Hidden2-\j);
                    \draw[->] (Input3-\i) -- (Hidden3-\j);
                }
            }

        \end{tikzpicture}
    \end{center}
    \begin{itemize}
        \item \textbf{Query (Q)}, \textbf{Key (K)}, \textbf{Value (V)}: projections différentes du même input
        \item Score d'attention: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
        \item Permet de capturer différents types de relations entre tokens
        \item Query représente la question, Key représente les réponses possibles, Value représente les réponses
    \end{itemize}
\end{frame}

% Slide 14: Récapitulation QKV
\begin{frame}{Récapitulation: Les matrices Query, Key et Value}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{center}
                %\includegraphics[width=\textwidth]{images/qkv.png}
                \begin{tikzpicture}[scale=0.9]
                    % Matrices
                    \draw[fill=yellow!20] (0,0) rectangle (1.5,3) node[pos=.5] {$Q$};
                    \draw[fill=red!20] (2,0) rectangle (3.5,3) node[pos=.5] {$K$};
                    \draw[fill=green!20] (4,0) rectangle (5.5,3) node[pos=.5] {$V$};
                    
                    % Dimensions
                    \draw[<->] (0,-0.5) -- (1.5,-0.5) node[midway, below] {$d_{emb}$};
                    \draw[<->] (2,-0.5) -- (3.5,-0.5) node[midway, below] {$d_{emb}$};
                    \draw[<->] (4,-0.5) -- (5.5,-0.5) node[midway, below] {$d_{emb}$};
                    \draw[<->] (-0.5,0) -- (-0.5,3) node[midway, left] {$C_{len}$};
                \end{tikzpicture}
            \end{center}
        \end{column}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Trois matrices de taille [contexte, embedding] ou une matrice de taille [contexte, 3 × embedding]
                \item \textbf{Queries (Q)} : Représentent ce que recherche chaque token
                \begin{itemize}
                    \item "Quelles informations sont importantes pour moi?"
                \end{itemize}
                \item \textbf{Keys (K)} : Encapsulent les informations disponibles dans chaque token
                \begin{itemize}
                    \item "Voici les informations que je peux fournir"
                \end{itemize}
                \item \textbf{Values (V)} : Contiennent les informations effectives à combiner
                \begin{itemize}
                    \item "Voici mon contenu qui sera utilisé pour la sortie"
                \end{itemize}
                \item Les scores d'attention (QK$^T$) déterminent quelles valeurs (V) sont pertinentes
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}


% Slide 15: Introduction au mécanisme d'attention
\begin{frame}{Le mécanisme d'attention: Comprendre les relations contextuelles}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.5\textwidth]{images/gpt2_attn.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item L'attention est le \textbf{cœur des Transformers}
                \item Permet de \textbf{pondérer dynamiquement} l'importance de chaque token
                \item Capture les \textbf{dépendances à longue distance}
                \item Chaque token peut "prêter attention" à tous les tokens précédents
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}


% Slide 16: Scaled Dot-Product Attention
\begin{frame}{Scaled Dot-Product Attention}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/scaled_dot_attn.png}
            \end{center}
        \end{column}
        \begin{column}{0.6\textwidth}
            \begin{align*}
                \text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \\
            \end{align*}
            \begin{itemize}
                \item $QK^T$ calcule les scores de similarité [contexte × contexte]
                \item Division par $\sqrt{d_k}$ stabilise les gradients
                \item \textbf{softmax} transforme les scores en poids [0,1]
                \item Multiplication par $V$ produit une moyenne pondérée des valeurs
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 16.5: Exemple de matrice d'attention
\begin{frame}{Exemple de matrice d'attention}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{images/attn_mat_example.png}
            \end{center}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{itemize}
                \item Visualisation des scores d'attention entre tokens
                \item Chaque ligne représente l'attention d'un token vers tous les autres
                \item Les zones plus foncées indiquent une attention plus forte
                \item Permet d'interpréter quelles parties du texte sont liées
                \item Révèle les relations grammaticales et sémantiques capturées
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}


% Slide 17: Multi-Head Attention
\begin{frame}{Multi-Head Attention: Attention parallèle à plusieurs niveaux}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/multi_head.png}
            \end{center}
        \end{column}
        \begin{column}{0.6\textwidth}
            \begin{align*}
                \text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \\
                \text{où head}_i &= \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
            \end{align*}
            \begin{itemize}
                \item Plusieurs "têtes" d'attention en parallèle
                \item Chaque tête se spécialise sur des aspects différents
                \begin{itemize}
                    \item Syntaxe, sémantique, coréférence, etc.
                \end{itemize}
                \item GPT-2 utilise 12 têtes, GPT-3 jusqu'à 96 têtes
                \item Les sorties des têtes sont concaténées puis projetées
                \item Permet de capturer plusieurs types de relations simultanément
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide: Architecture du Transformer original
\begin{frame}{Architecture du Transformer: Le modèle original encoder-decoder}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.6\textwidth, keepaspectratio]{images/transformers.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item \textbf{Architecture encoder-decoder}:
                \begin{itemize}
                    \item \textbf{Encoder}: Traite le texte source
                    \item \textbf{Decoder}: Génère le texte cible
                \end{itemize}
                \vspace{0.2cm}
                \item \textbf{Composants clés}:
                \begin{itemize}
                    \item Multi-head attention
                    \item Feed-forward networks
                    \item Residual connections
                    \item Layer normalization
                \end{itemize}
                \vspace{0.2cm}
                \item \textbf{Caractéristiques}:
                \begin{itemize}
                    \item Traitement parallèle des tokens
                    \item Attention bidirectionnelle (encoder)
                    \item Attention masquée (decoder)
                    \item Base de tous les modèles modernes
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 18: Post-Attention et MLP
\begin{frame}{Après l'attention: Feed-Forward Network}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{images/post_attn.png}
            \end{center}
        \end{column}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Après le mécanisme d'attention, vient un réseau feed-forward
                \item Appliqué indépendamment à chaque position (token)
                \item Composé de deux transformations linéaires et d'une activation
                \item Permet de transformer les représentations contextualisées
                \item Augmente la capacité de modélisation non-linéaire du réseau
                \item Contient la majorité des paramètres du Transformer
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 19: Structure détaillée du MLP
\begin{frame}{Structure du MLP dans les Transformers}
    \begin{center}
        \begin{tikzpicture}[>=latex, node distance=1.5cm, scale=0.65, transform shape]
            % Title for linear layer
            \node[text width=12cm, align=center] at (3,1) {\textbf{Projection linéaire dans le Feed-Forward Network}};
            
            % Inputs (reduced to 2)
            \foreach \i in {1,...,2}
            {
                \node (Input-\i) at (0,-\i*2) {$x_\i$};
            }

            % Hidden Layer
            \foreach \i in {1,...,6}
            {
                \node[draw, circle, minimum size=1cm] (Hidden-\i) at (3,{-\i*1.1+1}) {$f$};
            }

            % Output Layer (reduced to 2)
            \foreach \i in {1,...,2}
            {
                \node (Output-\i) at (6,-\i*2) {$y_\i$};
            }
            
            % Draw connections from inputs to the hidden layer and to the output layer
            \foreach \i in {1,...,2}
            {
                \foreach \j in {1,...,6}
                {
                    \draw[->] (Input-\i) -- (Hidden-\j);
                    \draw[->] (Hidden-\j) -- (Output-\i);
                }
            }

        \end{tikzpicture}
    \end{center}
    \vspace{-0.5cm}
    \begin{itemize}
        \item \textbf{Expansion}: La dimension cachée est 4 fois plus grande que l'entrée/sortie
        \item \textbf{GELU}: Fonction d'activation non-linéaire plus performante que ReLU
        \item \textbf{Projection}: Retour à la dimension d'origine
        \item \textbf{Dropout}: Régularisation pour éviter le surapprentissage
    \end{itemize}
\end{frame}

% Slide 20: Tête de classification
\begin{frame}{La tête de classification: Structure}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.7\textwidth]{images/gpt2_classif.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item \textbf{Dernière couche} du modèle LLM
                \item Transforme les représentations contextuelles en \textbf{probabilités} sur le vocabulaire
                \item Structure simple: \textbf{couche linéaire} suivie d'un \textbf{softmax}
                \item La dimension de sortie correspond à la \textbf{taille du vocabulaire} (50K-100K tokens)
                \item Dans GPT, la matrice de poids est souvent \textbf{partagée} avec la matrice d'embedding (weight tying)
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{La tête de classification: Prédiction du token suivant}
    \begin{itemize}
        \item \textbf{Adaptable} selon la tâche:
        \begin{itemize}
            \item Génération de texte: prédiction du token suivant
            \item Classification: prédiction d'une classe
            \item Question-réponse: extraction de réponses
        \end{itemize}
    \end{itemize}
    \vspace{0.5cm}
    \begin{center}
        \textbf{Fonction Softmax:} Conversion des logits en probabilités
        \begin{align*}
            P(token_i | contexte) = \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{|V|} e^{z_j}}
        \end{align*}
        où $z_i = (Wx + b)_i$ sont les logits pour le token $i$ et $|V|$ est la taille du vocabulaire
    \end{center}
\end{frame}

% Slide 21: Format du vecteur de sortie et masquage causal
\begin{frame}{Format de sortie et masquage causal}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{center}
                \begin{tikzpicture}[scale=0.75]
                    % Input tokens
                    \node[align=center] at (0,5.2) {Entrée};
                    \foreach \i/\word in {0/Le, 1/chat, 2/noir, 3/dort} {
                        \begin{scope}
                            \foreach \j in {0,...,\i} {
                                \pgfmathsetmacro{\wordindex}{min(\j,3)}
                                \pgfmathtruncatemacro{\wordindex}{\wordindex}
                                \pgfmathsetmacro{\wordtext}{{"Le","chat","noir","dort"}[\wordindex]}
                                \draw[fill=lightblue!40, draw=gray!60, rounded corners=2pt] (\j*2,4-\i*1) rectangle ++(1.8,0.7) node[pos=.5] {\wordtext};
                            }
                        \end{scope}
                    }
                    
                    % Output predictions with shifted indices
                    \node[align=center] at (6,5.2) {Sortie (prédictions)};
                    \foreach \i/\word in {0/chat, 1/noir, 2/dort, 3/sur} {
                        \draw[fill=lightgreen!40, draw=gray!60, rounded corners=2pt] (3+\i*2,4-\i*1) rectangle ++(1.8,0.7) node[pos=.5] {\word};
                    }
                    
                    % Attention mask
                    \node[align=center] at (3.5,0) {Masque d'attention};
                    
                    % Attention mask grid - positioned below inputs/outputs
                    \draw[fill=blue!20, draw=gray] (2,-1) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=red!20, draw=gray] (2.7,-1) rectangle ++(0.7,0.7) node[pos=.5] {0};
                    \draw[fill=red!20, draw=gray] (3.4,-1) rectangle ++(0.7,0.7) node[pos=.5] {0};
                    \draw[fill=red!20, draw=gray] (4.1,-1) rectangle ++(0.7,0.7) node[pos=.5] {0};
                    
                    \draw[fill=blue!20, draw=gray] (2,-1.7) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=blue!20, draw=gray] (2.7,-1.7) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=red!20, draw=gray] (3.4,-1.7) rectangle ++(0.7,0.7) node[pos=.5] {0};
                    \draw[fill=red!20, draw=gray] (4.1,-1.7) rectangle ++(0.7,0.7) node[pos=.5] {0};
                    
                    \draw[fill=blue!20, draw=gray] (2,-2.4) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=blue!20, draw=gray] (2.7,-2.4) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=blue!20, draw=gray] (3.4,-2.4) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=red!20, draw=gray] (4.1,-2.4) rectangle ++(0.7,0.7) node[pos=.5] {0};
                    
                    \draw[fill=blue!20, draw=gray] (2,-3.1) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=blue!20, draw=gray] (2.7,-3.1) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=blue!20, draw=gray] (3.4,-3.1) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    \draw[fill=blue!20, draw=gray] (4.1,-3.1) rectangle ++(0.7,0.7) node[pos=.5] {1};
                    
                    % Add arrows to show prediction alignment
                    \foreach \i in {0,...,3} {
                        \draw[->, thick, dashed, blue!60] (1.9+\i*2,4-\i*1+0.35) to[out=0,in=180] (3+\i*2,4-\i*1+0.35);
                    }
                \end{tikzpicture}
            \end{center}
        \end{column}
        \begin{column}{0.45\textwidth}
            \begin{itemize}
                \item Le modèle génère des prédictions pour \textbf{tous les tokens} simultanément
                \item Pour chaque position, on prédit le token \textbf{suivant}
                \item Un \textbf{masque causal} empêche chaque token de "voir le futur":
                \begin{itemize}
                    \item Le premier token ne voit que lui-même
                    \item Le deuxième voit le premier et lui-même
                    \item Etc.
                \end{itemize}
                \item La sortie est de la \textbf{même dimension} que l'entrée mais \textbf{décalée} d'une position
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 22: Introduction aux stratégies de génération
\begin{frame}{Stratégies de génération: Du vecteur de probabilités au texte}
    \begin{center}
        \Large{\textbf{Comment générer du texte à partir du vecteur de probabilités?}}
        
        \vspace{0.2cm}
        
        \begin{tikzpicture}[scale=0.9]
            % Input vector
            \draw[fill=lightblue!40, draw=gray!60, rounded corners=2pt] (1,0) rectangle (11,1) node[pos=.5] {Vecteur de probabilités sur le vocabulaire};
            
            % Arrow down
            \draw[->, thick] (6,0) -- (6,-1);
            
            % Decoding strategies
            \node[align=center] at (1,-1.5) {\textbf{Stratégies déterministes}};
            \node[align=center] at (11,-1.5) {\textbf{Stratégies stochastiques}};
            
            % Methods
            \node[align=center, fill=lightblue!20, draw=gray!60, rounded corners=2pt, text width=3cm] at (0,-2.5) {Greedy Search};
            \node[align=center, fill=lightblue!20, draw=gray!60, rounded corners=2pt, text width=3cm] at (2,-3.5) {Beam Search};
            
            \node[align=center, fill=lightgreen!20, draw=gray!60, rounded corners=2pt, text width=3cm] at (7,-2.5) {Sampling};
            \node[align=center, fill=lightgreen!20, draw=gray!60, rounded corners=2pt, text width=3cm] at (12.4,-3.5) {Top-K sampling};
            \node[align=center, fill=lightgreen!20, draw=gray!60, rounded corners=2pt, text width=3cm] at (8,-4.5) {Top-P sampling (Nucleus)};
        \end{tikzpicture}
    \end{center}
\end{frame}

% Slide 23: Greedy Search
\begin{frame}{Greedy Search: La méthode la plus simple}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/beam.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{align*}
                w_t = \underset{w}{\text{argmax}} \ p(w|w_{1:t-1})
            \end{align*}
            \begin{itemize}
                \item À chaque étape, choisir le token le \textbf{plus probable}
                \item Méthode \textbf{déterministe}: pour une entrée donnée, toujours la même sortie
                \item \textbf{Inconvénients}:
                \begin{itemize}
                    \item Pas de diversité
                    \item Tendance à se répéter
                    \item Peut ignorer des chemins globalement meilleurs
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
    \vspace{0.3cm}
    \begin{center}
        \footnotesize{Illustrations adaptées de Hugging Face: \url{https://huggingface.co/blog/how-to-generate}}
    \end{center}
\end{frame}

% Slide 24: Beam Search
\begin{frame}{Beam Search: Explorer plusieurs chemins en parallèle}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/beam.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Maintient les \textbf{k meilleures hypothèses} à chaque étape (beam width)
                \item Pour chacune, explore toutes les possibilités au pas suivant
                \item Sélectionne les \textbf{k meilleures} parmi les \textbf{k × V} combinaisons
                \item Score d'une séquence $Y = (y_1, ..., y_t)$:
                \begin{align*}
                    score(Y) = \sum_{i=1}^{t} \log p(y_i|y_{1:i-1})
                \end{align*}
                \item \textbf{Problèmes}:
                \begin{itemize}
                    \item Toujours déterministe
                    \item Peu de diversité (les chemins convergent)
                    \item Coûteux pour de grandes valeurs de k
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 25: Sampling
\begin{frame}{Sampling: Introduire de l'aléatoire dans la génération}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/sampling.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{align*}
                w_t \sim p(w|w_{1:t-1})
            \end{align*}
            \begin{itemize}
                \item Échantillonne un token selon la distribution de probabilité
                \item \textbf{Avantages}:
                \begin{itemize}
                    \item Génère des textes \textbf{variés}
                    \item Peut trouver des formulations créatives
                    \item Plus naturel pour la conversation
                \end{itemize}
                \item \textbf{Inconvénients}:
                \begin{itemize}
                    \item Peut générer des tokens improbables
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Température: Contrôler l'aléatoire dans le sampling}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{align*}
                p_T(w_i|w_{1:t-1}) = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}
            \end{align*}
            où $z_i$ sont les logits pour le token $i$
            
            \begin{center}
                \begin{tikzpicture}[scale=0.8]
                    % Axes
                    \draw[->] (0,0) -- (6,0) node[right] {Tokens};
                    \draw[->] (0,0) -- (0,4) node[above] {Probabilité};
                    
                    % T=0.5 (plus pointue)
                    \draw[blue, thick] plot[smooth, tension=0.7] coordinates {(0,0.1) (1,0.2) (2,3) (3,0.5) (4,0.3) (5,0.1)};
                    \node[blue] at (1.5,3.3) {$T=0.5$};
                    
                    % T=1 (normale)
                    \draw[black, thick] plot[smooth, tension=0.7] coordinates {(0,0.3) (1,0.6) (2,2) (3,0.8) (4,0.5) (5,0.3)};
                    \node[black] at (3,2.3) {$T=1$};
                    
                    % T=2 (plus plate)
                    \draw[red, thick] plot[smooth, tension=0.7] coordinates {(0,0.5) (1,0.8) (2,1.2) (3,1) (4,0.7) (5,0.5)};
                    \node[red] at (4.5,1.5) {$T=2$};
                \end{tikzpicture}
            \end{center}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{itemize}
                \item Contrôle la "netteté" de la distribution
                \item \textbf{Température basse} ($T < 1$):
                \begin{itemize}
                    \item Plus conservateur
                    \item Accentue les différences
                    \item Proche du greedy search
                \end{itemize}
                \item \textbf{Température haute} ($T > 1$):
                \begin{itemize}
                    \item Plus exploratoire
                    \item Aplatit la distribution
                    \item Génération plus diverse
                \end{itemize}
                \item $T = 0$: équivalent à greedy
                \item Hyperparamètre crucial pour la qualité de génération
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

% Slide 26: Top-K Sampling
\begin{frame}{Top-K Sampling: Limiter l'espace des possibles}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{images/top_k.png}
    \end{center}
    
    \begin{enumerate}
        \item Sélectionner les \textbf{K tokens} les plus probables
        \item \textbf{Renormaliser} leurs probabilités:
        \begin{align*}
            p(w|w_{1:t-1}) = 
            \begin{cases}
            \frac{p(w|w_{1:t-1})}{\sum_{w' \in \text{top-K}} p(w'|w_{1:t-1})} & \text{si } w \in \text{top-K} \\
            0 & \text{sinon}
            \end{cases}
        \end{align*}
        \item Échantillonner à partir de cette distribution tronquée
    \end{enumerate}
\end{frame}

\begin{frame}{Top-K Sampling: Avantages et inconvénients}
    \begin{itemize}
        \item \textbf{Avantages}:
        \begin{itemize}
            \item Évite les tokens improbables
            \item Équilibre entre diversité et qualité
        \end{itemize}
        \item \textbf{Inconvénients}:
        \begin{itemize}
            \item Le choix de K est arbitraire
            \item Inadapté si la distribution est plate ou très pointue
        \end{itemize}
    \end{itemize}
\end{frame}

% Slide 27: Top-P (Nucleus) Sampling
\begin{frame}{Top-P Sampling: Adapter dynamiquement le nombre de choix}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{images/top_p.png}
    \end{center}
    
    \begin{enumerate}
        \item Sélectionner le \textbf{plus petit ensemble de tokens} dont la probabilité cumulée dépasse \textbf{p} (nucleus):
        \begin{align*}
            V^{(p)} = \min_{V' \subset V} \left\{ V' \left| \sum_{w \in V'} p(w|w_{1:t-1}) \geq p \right. \right\}
        \end{align*}
        \item Renormaliser les probabilités sur cet ensemble
        \item Échantillonner parmi ces tokens
    \end{enumerate}
\end{frame}

\begin{frame}{Top-P Sampling: Avantages et inconvénients}
    \begin{itemize}
        \item \textbf{Avantages}:
        \begin{itemize}
            \item S'adapte à la \textbf{forme de la distribution}
            \item Plus de choix pour les distributions plates
            \item Moins de choix pour les distributions pointues
            \item Meilleur équilibre qualité/diversité
        \end{itemize}
        \item \textbf{Inconvénients}:
        \begin{itemize}
            \item Peu prévisible (nombre de tokens variable)
        \end{itemize}
    \end{itemize}
\end{frame}

% Nouvelles slides sur le pré-entraînement et le fine-tuning
\begin{frame}{Pré-entraînement vs Fine-tuning: Deux phases distinctes}
    \begin{itemize}
        \item \textbf{Pré-entraînement}:
        \begin{itemize}
            \item Entraînement sur des \textbf{données massives et générales}
            \item Objectif: apprendre des \textbf{représentations générales} du langage
            \item Tâche auto-supervisée: prédiction du token suivant
            \item Nécessite d'énormes ressources de calcul
            \item Réalisé une seule fois par les grands laboratoires
        \end{itemize}
        \vspace{0.3cm}
        \item \textbf{Fine-tuning}:
        \begin{itemize}
            \item Adaptation à une \textbf{tâche spécifique}
            \item Données \textbf{étiquetées} pour la tâche cible
            \item Préserve les connaissances générales
            \item Ressources de calcul plus modestes
            \item Meilleure performance sur la tâche cible
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Full Fine-tuning vs Partial Fine-tuning}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{images/generated/full_vs_partial_finetuning.png}
            \end{center}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{itemize}
                \item \textbf{Full Fine-tuning}:
                \begin{itemize}
                    \item Mise à jour de \textbf{tous les paramètres} du modèle
                    \item Performances optimales
                    \item Nécessite beaucoup de \textbf{mémoire} et de \textbf{calcul}
                    \item Risque d'\textbf{oubli catastrophique}
                \end{itemize}
                \vspace{0.3cm}
                \item \textbf{Partial Fine-tuning}:
                \begin{itemize}
                    \item Seules certaines couches sont entraînées (souvent les dernières)
                    \item Moins de paramètres à mettre à jour
                    \item Économie de ressources
                    \item Préserve mieux les connaissances générales
                    \item Idéal pour des tâches similaires
                \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Méthodes avancées de Fine-tuning: Efficacité et performances}
    \begin{itemize}
        \item \textbf{Adapter}: Petits modules d'adaptation insérés entre les couches figées
        \item \textbf{LoRA} (Low-Rank Adaptation): Matrices de rang faible en parallèle des poids principaux
        \item \textbf{Prompt Tuning}: Ajout de tokens apprenables au contexte d'entrée
        \item Avantages communs: économie de mémoire, transfert facilité, multitâche
    \end{itemize}
\end{frame}

\end{document}
