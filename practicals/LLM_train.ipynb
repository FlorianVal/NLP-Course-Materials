{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5a26d2",
   "metadata": {},
   "source": [
    "# TP : Implémentation d'un petit LLM avec PyTorch\n",
    "\n",
    "Ce TP vous guidera à travers les étapes de construction et d'entraînement d'un petit modèle de langage (LLM) en utilisant PyTorch. Vous apprendrez à :\n",
    "\n",
    "1.  Charger et préparer des données textuelles.\n",
    "2.  Implémenter une architecture Transformer simplifiée (type decoder).\n",
    "3.  Entraîner le modèle sur un corpus de texte.\n",
    "4.  Visualiser les cartes d'attention.\n",
    "5.  Générer du texte.\n",
    "6.  Implémenter une version simplifiée de RLHF pour interagir avec votre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7cab72",
   "metadata": {},
   "source": [
    "### Partie 1 : Chargement et préparation des données\n",
    "\n",
    "Pour ce TP, nous allons utiliser le dataset **\"rayml/french_gutenberg\"** disponible sur Hugging Face Datasets. Ce dataset contient un grand nombre de livres du projet Gutenberg en français. Nous allons charger une partie du dataset et effectuer un prétraitement minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74674eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances nécessaires\n",
    "!pip install torch datasets transformers tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEQUENCE_LENGTH = 128  # Longueur de séquence désirée\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer  # Utilisation de GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 1. Charger le dataset\n",
    "dataset = load_dataset(\"rayml/french_gutenberg\", split=\"train[:10%]\")\n",
    "\n",
    "# 2. Choisir une colonne de texte\n",
    "text_column = \"content\"\n",
    "\n",
    "# 3. Charger le tokenizer GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token # Très important pour le padding avec GPT2\n",
    "\n",
    "# 4. Fonction de tokenisation\n",
    "def tokenize_function(examples):\n",
    "    # Tokeniser le texte complet sans truncation\n",
    "    return tokenizer(examples[text_column], padding=False, truncation=False)\n",
    "\n",
    "# 5. Tokeniser le dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 6. Créer un Dataset PyTorch qui extrait des séquences de longueur fixe\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, seq_length):\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.stride = seq_length // 2  # Utiliser un stride égal à la moitié de la longueur de séquence\n",
    "        self.total_sequences = 0\n",
    "        self.cumulative_lengths = []\n",
    "        \n",
    "        # Calculer le nombre total de séquences possibles avec le stride modifié\n",
    "        for item in dataset:\n",
    "            num_sequences = max(0, (len(item['input_ids']) - seq_length) // self.stride + 1)\n",
    "            self.total_sequences += num_sequences\n",
    "            self.cumulative_lengths.append(self.total_sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Trouver dans quel texte se trouve la séquence demandée\n",
    "        text_idx = 0\n",
    "        while text_idx < len(self.cumulative_lengths) and idx >= self.cumulative_lengths[text_idx]:\n",
    "            text_idx += 1\n",
    "        \n",
    "        # Calculer la position de début dans ce texte\n",
    "        if text_idx == 0:\n",
    "            sequence_idx = idx\n",
    "        else:\n",
    "            sequence_idx = idx - self.cumulative_lengths[text_idx - 1]\n",
    "            \n",
    "        # Calculer la position de début en tenant compte du stride\n",
    "        start_pos = sequence_idx * self.stride\n",
    "            \n",
    "        # Extraire la séquence\n",
    "        input_ids = self.dataset[text_idx]['input_ids'][start_pos:start_pos + self.seq_length]\n",
    "        \n",
    "        # S'assurer que la séquence a la bonne longueur (important pour la dernière séquence)\n",
    "        if len(input_ids) < self.seq_length:\n",
    "            # Padding pour la dernière séquence si nécessaire\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * (self.seq_length - len(input_ids))\n",
    "            \n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        \n",
    "        # Créer le masque d'attention (1 pour les tokens réels, 0 pour le padding)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Créer les labels (décalés d'une position)\n",
    "        labels = input_ids[1:].clone()\n",
    "        labels = torch.cat([labels, torch.tensor([tokenizer.pad_token_id])])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "# 7. Créer un DataLoader PyTorch\n",
    "train_dataset = TextDataset(tokenized_dataset, SEQUENCE_LENGTH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Exemple d'utilisation du DataLoader\n",
    "batch = next(iter(train_dataloader))\n",
    "print(f\"Shape des input_ids : {batch['input_ids'].shape}\")\n",
    "print(f\"Shape du attention_mask : {batch['attention_mask'].shape}\")\n",
    "print(f\"Shape des labels : {batch['labels'].shape}\")\n",
    "\n",
    "# Afficher 2 séquences consécutives pour vérifier le chevauchement\n",
    "print(\"\\nVérification du chevauchement entre séquences consécutives:\")\n",
    "seq1 = train_dataset[0]  # Première séquence\n",
    "seq2 = train_dataset[1]  # Deuxième séquence (devrait avoir un chevauchement de 50%)\n",
    "\n",
    "# Décoder les tokens en texte\n",
    "text1 = tokenizer.decode(seq1['input_ids'])\n",
    "text2 = tokenizer.decode(seq2['input_ids'])\n",
    "\n",
    "print(\"\\nSéquence 1:\")\n",
    "print(text1)\n",
    "print(\"\\nSéquence 2:\")\n",
    "print(text2)\n",
    "\n",
    "# Vérifier la taille du chevauchement\n",
    "overlap_size = SEQUENCE_LENGTH // 2\n",
    "print(f\"\\nTaille théorique du chevauchement: {overlap_size} tokens (50% de {SEQUENCE_LENGTH})\")\n",
    "\n",
    "# Visualiser la première moitié de la séquence 2 (qui devrait correspondre à la deuxième moitié de la séquence 1)\n",
    "print(\"\\nDernière moitié de la séquence 1:\")\n",
    "print(tokenizer.decode(seq1['input_ids'][overlap_size:]))\n",
    "print(\"\\nPremière moitié de la séquence 2:\")\n",
    "print(tokenizer.decode(seq2['input_ids'][:overlap_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a3f55",
   "metadata": {},
   "source": [
    "### Partie 2 : Implémentation du modèle\n",
    "\n",
    "Nous allons implémenter un modèle de type Transformer (decoder) en PyTorch. L'implémentation sera faite bloc par bloc, en commençant par le bloc d'attention.\n",
    "\n",
    "#### 2.1 Bloc d'attention\n",
    "\n",
    "Le bloc d'attention est le cœur du modèle Transformer. Il permet au modèle de pondérer l'importance des différents mots de la séquence d'entrée lors du traitement d'un mot donné.\n",
    "\n",
    "##### 2.1.1 Projection en Q, K, V\n",
    "\n",
    "La première étape consiste à projeter les embeddings des mots d'entrée en trois vecteurs : Query (Q), Key (K) et Value (V). Ces projections sont effectuées à l'aide de couches linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model doit être divisible par num_heads\"\n",
    "\n",
    "        self.d_k = d_model // num_heads  # Dimension de chaque tête\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Couches linéaires pour Q, K et V\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Couche linéaire pour la sortie\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        attention_mask: (batch_size, seq_len)  (optionnel, pour le padding)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # 1. Projection en Q, K, V\n",
    "        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        k = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        v = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "        # 2. Calcul de l'attention\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Masque causal pour que le modèle n'attende pas sur le futur - Créé dynamiquement selon la séquence\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len).to(x.device)\n",
    "        attention_scores = attention_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "\n",
    "        # Appliquer le masque de padding si fourni\n",
    "        if attention_mask is not None:\n",
    "            # Reformater le masque pour qu'il soit compatible avec la forme des scores d'attention\n",
    "            # attention_mask: (batch_size, seq_len) -> (batch_size, 1, 1, seq_len)\n",
    "            padding_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            # Créer un masque complet pour tous les tokens cibles\n",
    "            # Pour chaque position source : si le token est masqué, toutes les positions cibles verront son attention masquée\n",
    "            padding_mask = padding_mask.expand(-1, -1, seq_len, -1)\n",
    "            attention_scores = attention_scores.masked_fill(padding_mask == 0, float('-inf'))\n",
    "\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # 3. Pondération des valeurs\n",
    "        attended_values = torch.matmul(attention_probs, v)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 4. Projection de sortie\n",
    "        output = self.W_o(attended_values)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        return output, attention_probs # Retourner aussi les probabilités d'attention pour la visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd650a93",
   "metadata": {},
   "source": [
    "##### 2.1.2 Calcul de l'attention\n",
    "\n",
    "L'attention est calculée en effectuant un produit scalaire entre les vecteurs Q et K, en appliquant une fonction softmax et en pondérant les vecteurs V en conséquence. On utilise aussi un masque causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6deed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.gelu = nn.GELU()  # Utiliser GELU au lieu de ReLU\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb366e6",
   "metadata": {},
   "source": [
    "#### 2.3 Tête de classification\n",
    "\n",
    "La tête de classification est une simple couche linéaire qui projette la sortie du modèle vers l'espace des tokens du vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7210f",
   "metadata": {},
   "source": [
    "#### 2.4 Modèle complet\n",
    "\n",
    "Nous pouvons maintenant assembler les différents blocs pour construire le modèle complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.layers = nn.ModuleList([layer for _ in range(num_layers) for layer in [MultiHeadAttention(d_model, num_heads), MLP(d_model, d_ff)]])\n",
    "        self.head = Head(d_model, vocab_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.register_buffer('position_ids', torch.arange(max_seq_len).expand((1, -1)))\n",
    "\n",
    "    def forward(self, idx, attention_mask=None):\n",
    "        \"\"\"\n",
    "        idx: (batch_size, seq_len)\n",
    "        attention_mask: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = idx.size()\n",
    "        assert seq_len <= self.max_seq_len, f\"Sequence length ({seq_len}) exceeds maximum sequence length ({self.max_seq_len})\"\n",
    "\n",
    "        # 1. Embeddings\n",
    "        token_embeddings = self.token_embedding(idx)  # (batch_size, seq_len, d_model)\n",
    "        position_embeddings = self.position_embedding(self.position_ids[:, :seq_len])  # (1, seq_len, d_model) -> (batch_size, seq_len, d_model)\n",
    "        x = token_embeddings + position_embeddings  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # 2. Blocs Transformer\n",
    "        all_attention_probs = [] # Pour stocker les poids d'attention de chaque couche\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                x, attention_probs = layer(x, attention_mask) # Récupérer les poids d'attention\n",
    "                all_attention_probs.append(attention_probs)\n",
    "            else:\n",
    "                x = layer(x) # Si c'est un MLP, on ne récupère pas les poids\n",
    "\n",
    "        # 3. Tête de classification\n",
    "        logits = self.head(x)  # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return logits, all_attention_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cd9e6",
   "metadata": {},
   "source": [
    "### Partie 3 : Entraînement du modèle\n",
    "\n",
    "Nous allons maintenant instancier le modèle, définir la fonction de perte et l'optimiseur, et lancer l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Instancier le modèle\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "d_model = 256  # Dimension de l'embedding et des hidden states\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "d_ff = 512  # Dimension de la couche feedforward\n",
    "max_seq_len = 128\n",
    "\n",
    "model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len)\n",
    "\n",
    "# 2. Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) # Ignorer le padding dans le calcul de la loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 3. Définir le nombre d'epochs\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 4. Boucle d'entraînement\n",
    "best_loss = float('inf')\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False) # Ajouter une barre de progression\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(input_ids, attention_mask) # Ne pas oublier de passer le attention_mask\n",
    "        # Les logits ont la forme (batch_size, seq_len, vocab_size)\n",
    "        # La loss CrossEntropy attend (batch_size * seq_len, vocab_size) et (batch_size * seq_len)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()}) # Mise à jour de la loss dans la barre de progression\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Sauvegarder le meilleur modèle\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': avg_loss,\n",
    "            'vocab_size': vocab_size,\n",
    "            'd_model': d_model,\n",
    "            'num_heads': num_heads,\n",
    "            'num_layers': num_layers,\n",
    "            'd_ff': d_ff,\n",
    "            'max_seq_len': max_seq_len,\n",
    "        }, 'best_model_checkpoint.pth')\n",
    "        print(f\"Meilleur modèle sauvegardé avec une loss de {avg_loss:.4f}\")\n",
    "    \n",
    "    # Sauvegarde régulière\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': avg_loss,\n",
    "            'vocab_size': vocab_size,\n",
    "            'd_model': d_model,\n",
    "            'num_heads': num_heads,\n",
    "            'num_layers': num_layers,\n",
    "            'd_ff': d_ff,\n",
    "            'max_seq_len': max_seq_len,\n",
    "        }, f'model_checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Fonction pour charger un modèle sauvegardé\n",
    "def load_model(checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Recréer le modèle avec les mêmes hyperparamètres\n",
    "    model = Transformer(\n",
    "        checkpoint['vocab_size'],\n",
    "        checkpoint['d_model'],\n",
    "        checkpoint['num_heads'],\n",
    "        checkpoint['num_layers'],\n",
    "        checkpoint['d_ff'],\n",
    "        checkpoint['max_seq_len']\n",
    "    )\n",
    "    \n",
    "    # Charger les poids\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 5. Visualiser la loss\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3840a",
   "metadata": {},
   "source": [
    "### Explications\n",
    "\n",
    "* **Instanciation du modèle :** Nous instancions le modèle `Transformer` avec les hyperparamètres choisis. Nous récupérons la taille du vocabulaire à partir du tokenizer.\n",
    "\n",
    "* **Fonction de perte et optimiseur :** Nous utilisons la `CrossEntropyLoss` et l'optimiseur `AdamW`. Il est important d'ignorer le padding lors du calcul de la loss.\n",
    "\n",
    "* **Boucle d'entraînement :**\n",
    "\n",
    "    * Nous entraînons le modèle pendant le nombre d'epochs spécifié.\n",
    "\n",
    "    * Nous utilisons une boucle `tqdm` pour afficher une barre de progression pendant l'entraînement.\n",
    "\n",
    "    * Pour chaque batch :\n",
    "\n",
    "        * Nous déplaçons les données sur le GPU si disponible.\n",
    "\n",
    "        * Nous effectuons une passe forward, calculons la loss, effectuons la rétropropagation et mettons à jour les poids.\n",
    "\n",
    "        * Nous stockons la loss pour la visualiser plus tard.\n",
    "\n",
    "* **Visualisation de la loss :** Nous utilisons Matplotlib pour tracer la courbe de la loss d'entraînement.\n",
    "\n",
    "N'oubliez pas de lancer le code dans un environnement avec PyTorch installé. La loss devrait décroître au fil des epochs, indiquant que le modèle apprend.\n",
    "\n",
    "### Partie 4 : Visualisation des cartes d'attention\n",
    "\n",
    "Maintenant que notre modèle est entraîné, nous allons visualiser les cartes d'attention pour mieux comprendre comment il traite l'information. Cette visualisation nous permettra de voir quels mots le modèle considère comme importants lorsqu'il prédit le mot suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597881fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fonction pour visualiser les cartes d'attention\n",
    "def plot_attention_maps(model, text, tokenizer, device):\n",
    "    # Tokeniser le texte\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Créer un masque d'attention\n",
    "    attention_mask = torch.ones_like(token_ids)\n",
    "    \n",
    "    # Obtenir les probabilités d'attention\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, attention_probs = model(token_ids, attention_mask)\n",
    "    \n",
    "    # Convertir les tokens en texte lisible\n",
    "    token_texts = [tokenizer.decode([id]) for id in token_ids[0]]\n",
    "    \n",
    "    # Afficher les cartes d'attention pour chaque couche et chaque tête\n",
    "    num_layers = len(attention_probs) // 2  # Diviser par 2 car chaque couche contient une attention et un MLP\n",
    "    \n",
    "    fig, axs = plt.subplots(num_layers, model.layers[0].num_heads, \n",
    "                           figsize=(model.layers[0].num_heads * 3, num_layers * 3))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        axs = np.array([axs])  # Convertir en tableau 2D pour indexation uniforme\n",
    "    \n",
    "    layer_idx = 0\n",
    "    for i, layer_attn in enumerate(attention_probs):\n",
    "        # Les attention_probs sont dans les couches d'attention (pas dans les MLP)\n",
    "        for head in range(model.layers[0].num_heads):\n",
    "            # Récupérer les scores d'attention pour cette tête\n",
    "            attn = layer_attn[0, head, :len(token_texts), :len(token_texts)].cpu().numpy()\n",
    "            \n",
    "            # Créer un heatmap\n",
    "            sns.heatmap(attn, \n",
    "                      cmap='viridis', \n",
    "                      xticklabels=token_texts,\n",
    "                      yticklabels=token_texts,\n",
    "                      ax=axs[layer_idx, head])\n",
    "            \n",
    "            axs[layer_idx, head].set_title(f'Couche {layer_idx+1}, Tête {head+1}')\n",
    "            plt.setp(axs[layer_idx, head].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        \n",
    "        layer_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Exemple de phrase à analyser\n",
    "test_text = \"Le petit chat noir dort sur le canapé.\"\n",
    "\n",
    "# Visualiser les cartes d'attention\n",
    "plot_attention_maps(model, test_text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d5e21",
   "metadata": {},
   "source": [
    "Les cartes d'attention montrent comment chaque mot est lié aux autres mots dans la phrase. Plus la couleur est vive, plus l'attention est forte. Voici quelques observations intéressantes à rechercher :\n",
    "\n",
    "1. **Motifs diagonaux** : Indiquent une attention sur le mot lui-même ou ses voisins immédiats.\n",
    "2. **Motifs verticaux** : Montrent quels mots sont les plus influents dans la phrase.\n",
    "3. **Spécialisation des têtes** : Certaines têtes peuvent se spécialiser dans les relations grammaticales, d'autres dans les relations sémantiques.\n",
    "4. **Différences entre couches** : Les couches inférieures capturent souvent des informations syntaxiques, tandis que les couches supérieures captent des informations plus sémantiques.\n",
    "\n",
    "Vous pouvez essayer avec différentes phrases pour voir comment le modèle réagit à différents types de texte.\n",
    "\n",
    "### Partie 5 : Génération de texte\n",
    "\n",
    "Maintenant que notre modèle est entraîné, nous allons l'utiliser pour générer du texte. La génération se fait de manière auto-régressive, c'est-à-dire que nous utilisons les prédictions précédentes comme entrée pour les prédictions suivantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112eac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0, \n",
    "                 top_k=0, greedy=False, device='cpu'):\n",
    "    \"\"\"\n",
    "    Génère du texte à partir d'une amorce (prompt).\n",
    "    \n",
    "    Args:\n",
    "        model: Le modèle entraîné\n",
    "        tokenizer: Le tokenizer utilisé\n",
    "        prompt: L'amorce (texte initial)\n",
    "        max_length: Nombre maximum de tokens à générer\n",
    "        temperature: Contrôle la randomisation (1.0 = normal, <1.0 = plus conservateur, >1.0 = plus aléatoire)\n",
    "        top_k: Si >0, limite le choix aux top_k tokens les plus probables\n",
    "        greedy: Si True, utilise une sélection déterministe (toujours le token le plus probable)\n",
    "        device: Le device (cpu ou cuda)\n",
    "    \n",
    "    Returns:\n",
    "        Le texte généré\n",
    "    \"\"\"\n",
    "    model.eval()  # Mode évaluation\n",
    "    \n",
    "    # Tokenisation de l'amorce\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Position initiale (longueur de l'amorce)\n",
    "    cur_len = input_ids.shape[1]\n",
    "    \n",
    "    # Générer jusqu'à atteindre max_length\n",
    "    for _ in range(max_length):\n",
    "        # Créer un masque d'attention (tout à 1)\n",
    "        attention_mask = torch.ones(1, cur_len, device=device)\n",
    "        \n",
    "        # Obtenir les prédictions du modèle\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(input_ids, attention_mask)\n",
    "        \n",
    "        # On ne s'intéresse qu'à la prédiction pour le dernier token\n",
    "        next_token_logits = outputs[0, -1, :]\n",
    "        \n",
    "        # Appliquer température\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Stratégie de génération\n",
    "        if greedy:\n",
    "            # Greedy decoding (prendre le token le plus probable)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        else:\n",
    "            # Sampling avec ou sans top-k\n",
    "            if top_k > 0:\n",
    "                # Garder uniquement les top_k tokens les plus probables\n",
    "                indices_to_remove = torch.topk(next_token_logits, top_k)[1]\n",
    "                mask = torch.ones_like(next_token_logits, dtype=torch.bool)\n",
    "                mask[indices_to_remove] = False\n",
    "                next_token_logits[mask] = float('-inf')\n",
    "            \n",
    "            # Appliquer softmax pour obtenir une distribution de probabilités\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Échantillonner un token selon cette distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Ajouter le token généré à l'entrée\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "        cur_len += 1\n",
    "        \n",
    "        # Arrêter si on génère un token de fin de séquence\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Décoder les tokens en texte\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Amorce littéraire\n",
    "prompt = \"Il était une fois, dans une forêt sombre et mystérieuse,\"\n",
    "\n",
    "# Différentes stratégies de génération\n",
    "print(\"Génération greedy (déterministe):\")\n",
    "greedy_text = generate_text(model, tokenizer, prompt, max_length=50, greedy=True, device=device)\n",
    "print(greedy_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Génération avec sampling (temperature=0.7):\")\n",
    "sampling_text = generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7, device=device)\n",
    "print(sampling_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Génération avec top-k sampling (k=40, temperature=0.9):\")\n",
    "topk_text = generate_text(model, tokenizer, prompt, max_length=50, temperature=0.9, top_k=40, device=device)\n",
    "print(topk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1269f",
   "metadata": {},
   "source": [
    "Vous pouvez expérimenter avec différentes amorces et paramètres de génération :\n",
    "\n",
    "1. **Temperature** : \n",
    "   - Valeur basse (ex. 0.3) : texte plus prévisible et répétitif\n",
    "   - Valeur haute (ex. 1.5) : texte plus aléatoire et créatif\n",
    "\n",
    "2. **Top-k** : \n",
    "   - Valeur basse (ex. 10) : limite fortement les choix possibles\n",
    "   - Valeur haute (ex. 50) : permet plus de diversité\n",
    "\n",
    "3. **Amorces** :\n",
    "   - Essayez différents débuts d'histoire\n",
    "   - Variez le style (conte, roman, poésie, etc.)\n",
    "   - Posez des questions pour voir comment le modèle y répond\n",
    "\n",
    "Plus vous avez entraîné longtemps votre modèle, plus la qualité du texte généré sera élevée. Il est également intéressant de comparer les différentes stratégies de génération sur la même amorce pour voir les variations dans le style et la cohérence du texte produit.\n",
    "\n",
    "### Partie 6 : Interagir avec le modèle grâce à un simple RLHF\n",
    "\n",
    "Dans cette partie, nous allons implémenter une version très simplifiée du RLHF (Reinforcement Learning from Human Feedback) pour permettre une interaction avec notre modèle et l'améliorer en fonction de nos retours. Cette technique est à la base des modèles comme ChatGPT et Claude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a598da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "class SimpleRLHF:\n",
    "    def __init__(self, model, tokenizer, device, learning_rate=1e-5):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.conversations = []  # Pour stocker l'historique des conversations\n",
    "        self.rewards = []  # Pour stocker les récompenses\n",
    "        \n",
    "    def generate_response(self, prompt, max_length=50, temperature=0.7, top_k=40):\n",
    "        \"\"\"Génère une réponse à partir d'un prompt\"\"\"\n",
    "        return generate_text(self.model, self.tokenizer, prompt, \n",
    "                            max_length=max_length, \n",
    "                            temperature=temperature,\n",
    "                            top_k=top_k, \n",
    "                            device=self.device)\n",
    "    \n",
    "    def update_model(self, prompt, response, reward):\n",
    "        \"\"\"Met à jour le modèle en fonction de la récompense\"\"\"\n",
    "        # Tokeniser l'entrée et la sortie\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "        target_ids = self.tokenizer.encode(response, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        # Créer des masques d'attention\n",
    "        input_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Calculer les logits pour l'entrée\n",
    "        logits, _ = self.model(input_ids, input_mask)\n",
    "        \n",
    "        # Créer une target avec les probabilités de reward plus élevées pour les tokens de la réponse\n",
    "        # Version simplifiée: augmenter ou diminuer les probabilités selon la récompense\n",
    "        reward_factor = reward  # Entre -1 et 1\n",
    "        \n",
    "        # Préparer les labels\n",
    "        labels = torch.zeros(logits.shape[0], logits.shape[1], logits.shape[2]).to(self.device)\n",
    "        \n",
    "        # Encourager ou décourager les tokens de la réponse selon la récompense\n",
    "        for i in range(min(target_ids.shape[1], logits.shape[1])):\n",
    "            if i < target_ids.shape[1]:\n",
    "                target_idx = target_ids[0, i].item()\n",
    "                labels[0, i, target_idx] = 1.0 * (1 + reward_factor)\n",
    "        \n",
    "        # Calculer la loss (KL divergence simplifiée)\n",
    "        loss = -torch.sum(labels * torch.log_softmax(logits, dim=-1))\n",
    "        \n",
    "        # Rétropropagation\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def chat(self, initial_prompt=\"Bonjour, comment puis-je vous aider?\", turns=5):\n",
    "        \"\"\"Interface de chat interactive avec feedback utilisateur\"\"\"\n",
    "        conversation = [initial_prompt]\n",
    "        print(f\"Modèle: {initial_prompt}\")\n",
    "        \n",
    "        for i in range(turns):\n",
    "            # Obtenir l'entrée utilisateur\n",
    "            user_input = input(\"Vous: \")\n",
    "            conversation.append(user_input)\n",
    "            \n",
    "            # Générer une réponse\n",
    "            full_prompt = \" \".join(conversation)\n",
    "            response = self.generate_response(full_prompt)\n",
    "            \n",
    "            # Extraire seulement la partie générée (après la dernière entrée utilisateur)\n",
    "            response_only = response[len(full_prompt):]\n",
    "            \n",
    "            # Afficher la réponse\n",
    "            print(f\"Modèle: {response_only}\")\n",
    "            conversation.append(response_only)\n",
    "            \n",
    "            # Demander un feedback\n",
    "            while True:\n",
    "                feedback = input(\"Votre feedback (-1 à +1) où +1 est excellent: \")\n",
    "                try:\n",
    "                    reward = float(feedback)\n",
    "                    if -1 <= reward <= 1:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Veuillez entrer une valeur entre -1 et 1\")\n",
    "                except ValueError:\n",
    "                    print(\"Veuillez entrer un nombre valide\")\n",
    "            \n",
    "            # Mettre à jour le modèle avec le feedback\n",
    "            loss = self.update_model(full_prompt, response_only, reward)\n",
    "            \n",
    "            # Stocker pour analyse\n",
    "            self.conversations.append((full_prompt, response_only))\n",
    "            self.rewards.append(reward)\n",
    "            \n",
    "            print(f\"Modèle mis à jour. Loss: {loss:.4f}\")\n",
    "            \n",
    "        # Sauvegarder le modèle après la session de RLHF\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'vocab_size': self.model.head.fc.out_features,\n",
    "            'd_model': self.model.d_model,\n",
    "            'num_heads': self.model.layers[0].num_heads,\n",
    "            'num_layers': len([l for l in self.model.layers if isinstance(l, MultiHeadAttention)]),\n",
    "            'd_ff': self.model.layers[1].fc1.out_features,\n",
    "            'max_seq_len': self.model.max_seq_len,\n",
    "        }, 'rlhf_tuned_model.pth')\n",
    "        \n",
    "        print(\"Session de chat terminée et modèle sauvegardé.\")\n",
    "        \n",
    "    def plot_rewards(self):\n",
    "        \"\"\"Affiche un graphique des récompenses obtenues\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.rewards, marker='o')\n",
    "        plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "        plt.title('Évolution des récompenses')\n",
    "        plt.xlabel('Interaction')\n",
    "        plt.ylabel('Récompense')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Création de l'instance RLHF\n",
    "rlhf_trainer = SimpleRLHF(model, tokenizer, device)\n",
    "\n",
    "# Démarrer une session de chat interactive\n",
    "print(\"Commençons une conversation. Donnez un feedback après chaque réponse du modèle.\")\n",
    "rlhf_trainer.chat(initial_prompt=\"Bonjour! Je suis un assistant littéraire. Comment puis-je vous aider aujourd'hui?\")\n",
    "\n",
    "# Visualiser l'évolution des récompenses\n",
    "rlhf_trainer.plot_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cbbc9",
   "metadata": {},
   "source": [
    "Dans cette implémentation simplifiée du RLHF :\n",
    "\n",
    "1. Nous avons créé une classe `SimpleRLHF` qui permet d'interagir avec notre modèle.\n",
    "2. À chaque tour de conversation, l'utilisateur donne une note entre -1 et +1 pour évaluer la réponse du modèle.\n",
    "3. Le modèle est mis à jour en fonction de cette récompense, en renforçant les tokens qui ont conduit à des réponses bien notées et en affaiblissant ceux des réponses mal notées.\n",
    "4. L'historique des conversations et des récompenses est enregistré, et nous pouvons visualiser l'évolution des performances du modèle.\n",
    "\n",
    "Ce processus est une version très simplifiée du RLHF utilisé pour former des modèles comme ChatGPT, mais il illustre le principe fondamental : les modèles peuvent apprendre de nos retours pour s'améliorer progressivement.\n",
    "\n",
    "Essayez de discuter avec votre modèle et observez s'il s'améliore au fur et à mesure des interactions !\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Dans ce TP, nous avons :\n",
    "1. Chargé et préparé un corpus de texte français\n",
    "2. Implémenté une architecture Transformer simplifiée\n",
    "3. Entraîné notre modèle sur le corpus\n",
    "4. Visualisé les mécanismes d'attention\n",
    "5. Utilisé le modèle pour générer du texte\n",
    "6. Implémenté une version simplifiée de RLHF pour interagir avec notre modèle\n",
    "\n",
    "Ce petit LLM est bien sûr très simple comparé aux modèles comme GPT-3/4 ou LLaMA, mais il illustre les principes fondamentaux qui régissent ces modèles plus avancés. Pour aller plus loin, vous pourriez :\n",
    "\n",
    "- Augmenter la taille du modèle (couches, têtes d'attention, dimension d'embedding)\n",
    "- Entraîner sur un corpus plus large\n",
    "- Améliorer l'algorithme de RLHF avec une fonction de récompense plus sophistiquée\n",
    "- Ajouter des mécanismes comme l'instruction-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
