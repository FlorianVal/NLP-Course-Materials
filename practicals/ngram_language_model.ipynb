{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5f787f",
   "metadata": {},
   "source": [
    "# TP 2: Modèles de Langage N-gram avec Tokenisation BPE\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans ce TP, nous allons mettre en pratique les concepts de tokenisation et de modèles de langage n-gram vus en cours. Nous utiliserons un dataset de textes de Shakespeare pour:\n",
    "\n",
    "1. Analyser les données textuelles\n",
    "2. Implémenter et entraîner un tokenizer BPE (Byte Pair Encoding)\n",
    "3. Construire un modèle de langage n-gram\n",
    "4. Générer du texte à partir de notre modèle\n",
    "\n",
    "Ce TP vous permettra de comprendre concrètement comment fonctionne la génération de texte à partir d'approches statistiques, avant d'aborder les modèles neuronaux plus complexes.\n",
    "\n",
    "## Partie 1: Configuration et chargement des données\n",
    "\n",
    "Commençons par importer les bibliothèques nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55b9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fb61b",
   "metadata": {},
   "source": [
    "Téléchargeons et chargeons notre corpus de Shakespeare :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd50737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL du dataset Shakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# Téléchargement du dataset\n",
    "response = requests.get(url)\n",
    "shakespeare_text = response.text\n",
    "\n",
    "# Affichage d'un extrait\n",
    "print(f\"Longueur du texte: {len(shakespeare_text)} caractères\")\n",
    "print(\"\\nExtrait du texte:\")\n",
    "print(shakespeare_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12888d4",
   "metadata": {},
   "source": [
    "## Partie 2: Analyse exploratoire des données\n",
    "\n",
    "Analysons notre corpus pour mieux comprendre sa structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d70de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de caractères uniques\n",
    "unique_chars = set(shakespeare_text)\n",
    "print(f\"Nombre de caractères uniques: {len(unique_chars)}\")\n",
    "print(f\"Caractères uniques: {''.join(sorted(unique_chars))}\")\n",
    "\n",
    "# Fréquence des caractères\n",
    "char_freq = Counter(shakespeare_text)\n",
    "print(\"\\nLes 10 caractères les plus fréquents:\")\n",
    "for char, count in char_freq.most_common(10):\n",
    "    if char == '\\n':\n",
    "        char_display = '\\\\n'\n",
    "    elif char == ' ':\n",
    "        char_display = 'espace'\n",
    "    else:\n",
    "        char_display = char\n",
    "    print(f\"'{char_display}': {count} occurrences\")\n",
    "\n",
    "# Nombre de mots (approximatif)\n",
    "words = re.findall(r'\\b\\w+\\b', shakespeare_text.lower())\n",
    "unique_words = set(words)\n",
    "print(f\"\\nNombre total de mots: {len(words)}\")\n",
    "print(f\"Nombre de mots uniques: {len(unique_words)}\")\n",
    "print(\"\\nLes 10 mots les plus fréquents:\")\n",
    "word_freq = Counter(words)\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"'{word}': {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f26d7",
   "metadata": {},
   "source": [
    "## Partie 3: Implémentation du tokenizer BPE\n",
    "\n",
    "Maintenant, implémentons notre propre tokenizer BPE (Byte Pair Encoding) pour comprendre comment il fonctionne. Comme vu en cours, BPE fonctionne en fusionnant progressivement les paires de tokens les plus fréquentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ac120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}  # Vocabulaire: token -> id\n",
    "        self.inverse_vocab = {}  # Id -> token\n",
    "        self.merges = []  # Liste des fusions BPE\n",
    "        self.char_level_tokens = True  # Indique si le tokenizer est au niveau des caractères\n",
    "    \n",
    "    def train(self, text: str, vocab_size: int, verbose: bool = True) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Entraîne le tokenizer BPE sur un texte donné jusqu'à atteindre la taille \n",
    "        de vocabulaire souhaitée.\n",
    "        \n",
    "        Args:\n",
    "            text: Texte d'entraînement\n",
    "            vocab_size: Taille cible du vocabulaire\n",
    "            verbose: Affiche des informations sur l'entraînement\n",
    "            \n",
    "        Returns:\n",
    "            Liste des fusions effectuées\n",
    "        \"\"\"\n",
    "        # Initialisation avec un vocabulaire de caractères\n",
    "        unique_chars = sorted(set(text))\n",
    "        self.vocab = {char: i for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        \n",
    "        # Tokenisation initiale du texte (chaque caractère est un token)\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Statistiques pour visualisation\n",
    "        vocab_sizes = [len(self.vocab)]\n",
    "        token_counts = [len(tokens)]\n",
    "        \n",
    "        # Effectuer les fusions jusqu'à atteindre la taille du vocabulaire cible\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            # Compter les paires de tokens adjacentes\n",
    "            pairs = self._count_pairs(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            # Trouver la paire la plus fréquente\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # À COMPLÉTER: Fusionner la paire la plus fréquente dans le texte\n",
    "            # ---------------------\n",
    "    \n",
    "            # ---------------------\n",
    "            \n",
    "            # Collecter des statistiques\n",
    "            vocab_sizes.append(len(self.vocab))\n",
    "            token_counts.append(len(tokens))\n",
    "            \n",
    "            if verbose and len(self.vocab) % 100 == 0:\n",
    "                print(f\"Vocabulaire: {len(self.vocab)}, Fusion: {best_pair} -> {new_token}\")\n",
    "            \n",
    "            # Nous ne sommes plus au niveau des caractères\n",
    "            self.char_level_tokens = False\n",
    "            \n",
    "        # Visualiser l'évolution du vocabulaire et du nombre de tokens\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(vocab_sizes)\n",
    "        plt.xlabel('Nombre de fusions')\n",
    "        plt.ylabel('Taille du vocabulaire')\n",
    "        plt.title('Évolution de la taille du vocabulaire')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(token_counts)\n",
    "        plt.xlabel('Nombre de fusions')\n",
    "        plt.ylabel('Nombre de tokens')\n",
    "        plt.title('Évolution du nombre de tokens')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.merges\n",
    "    \n",
    "    def _count_pairs(self, tokens: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"Compte les occurrences de chaque paire de tokens adjacents.\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i+1])\n",
    "            pairs[pair] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_pair(self, tokens: List[str], pair: Tuple[str, str], new_token: str) -> List[str]:\n",
    "        \"\"\"Fusionne toutes les occurrences d'une paire dans une liste de tokens.\"\"\"\n",
    "        # À COMPLÉTER: Implémenter la fusion des paires\n",
    "        # ---------------------\n",
    "        \n",
    "        # ---------------------\n",
    "        return result\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode un texte en utilisant le tokenizer BPE entraîné.\n",
    "        \n",
    "        Args:\n",
    "            text: Texte à encoder\n",
    "            \n",
    "        Returns:\n",
    "            Liste des IDs de tokens\n",
    "        \"\"\"\n",
    "        # Initialisation avec des caractères individuels\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Appliquer les fusions dans l'ordre où elles ont été apprises\n",
    "        for pair in self.merges:\n",
    "            tokens = self._merge_pair(tokens, pair, ''.join(pair))\n",
    "        \n",
    "        # Convertir les tokens en IDs\n",
    "        ids = [self.vocab.get(token, self.vocab.get(\"<unk>\", 0)) for token in tokens]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Décode une liste d'IDs de tokens en texte.\n",
    "        \n",
    "        Args:\n",
    "            ids: Liste des IDs de tokens\n",
    "            \n",
    "        Returns:\n",
    "            Texte décodé\n",
    "        \"\"\"\n",
    "        # Convertir les IDs en tokens\n",
    "        tokens = [self.inverse_vocab.get(id, \"<unk>\") for id in ids]\n",
    "        \n",
    "        # Concaténer les tokens\n",
    "        text = ''.join(tokens)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15568f38",
   "metadata": {},
   "source": [
    "Maintenant, entraînons notre tokenizer BPE sur le corpus de Shakespeare :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04494b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du tokenizer BPE\n",
    "tokenizer = BPETokenizer()\n",
    "vocab_size = 1000  # Taille cible du vocabulaire\n",
    "\n",
    "merges = tokenizer.train(shakespeare_text, vocab_size)\n",
    "\n",
    "# Encodons un extrait pour tester\n",
    "sample_text = shakespeare_text[:200]\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Texte original: {sample_text}\")\n",
    "print(f\"Texte encodé (IDs): {encoded[:20]}...\")\n",
    "print(f\"Texte décodé: {decoded}\")\n",
    "print(f\"Taille du vocabulaire final: {len(tokenizer.vocab)}\")\n",
    "print(f\"Nombre de tokens pour l'extrait: {len(encoded)}\")\n",
    "print(f\"Nombre de caractères de l'extrait: {len(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e7b51",
   "metadata": {},
   "source": [
    "## Partie 4: Implémentation du modèle n-gram\n",
    "\n",
    "Maintenant que nous avons notre tokenizer, implémentons un modèle de langage n-gram. Comme vu en cours, un modèle n-gram prédit le token suivant en se basant sur les n-1 tokens précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ddc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, n: int, tokenizer: BPETokenizer):\n",
    "        \"\"\"\n",
    "        Initialise un modèle de langage n-gram\n",
    "        \n",
    "        Args:\n",
    "            n: Ordre du modèle n-gram (nombre de tokens à considérer)\n",
    "            tokenizer: Tokenizer BPE entraîné\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.vocab)\n",
    "        self.context_counts = defaultdict(int)  # Compte des contextes (n-1 tokens)\n",
    "        self.ngram_counts = defaultdict(int)    # Compte des n-grams complets\n",
    "    \n",
    "    def train(self, text: str):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle n-gram sur un texte\n",
    "        \n",
    "        Args:\n",
    "            text: Texte d'entraînement\n",
    "        \"\"\"\n",
    "        # Tokeniser le texte\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "        \n",
    "        # À COMPLÉTER: Compter les n-grams et leurs contextes\n",
    "        # ---------------------\n",
    "        \n",
    "        # ---------------------\n",
    "            \n",
    "            # Incrémenter les compteurs\n",
    "            self.context_counts[context] += 1\n",
    "            self.ngram_counts[ngram] += 1\n",
    "    \n",
    "    def get_probability(self, context: Tuple[int], next_token: int) -> float:\n",
    "        \"\"\"\n",
    "        Calcule la probabilité P(next_token | context)\n",
    "        \n",
    "        Args:\n",
    "            context: Tuple des (n-1) token_ids précédents\n",
    "            next_token: Token_id suivant à prédire\n",
    "            \n",
    "        Returns:\n",
    "            Probabilité du token suivant étant donné le contexte\n",
    "        \"\"\"\n",
    "        # À COMPLÉTER: Calculer la probabilité conditionnelle\n",
    "        # ---------------------\n",
    "        \n",
    "        # ---------------------\n",
    "    \n",
    "    def generate_text(self, seed_text: str, length: int, temperature: float = 1.0) -> str:\n",
    "        \"\"\"\n",
    "        Génère du texte à partir d'un texte d'amorce (seed)\n",
    "        \n",
    "        Args:\n",
    "            seed_text: Texte d'amorce\n",
    "            length: Nombre de tokens à générer\n",
    "            temperature: Contrôle la randomisation (1.0 = fidèle aux probabilités,\n",
    "                        < 1.0 = plus conservateur, > 1.0 = plus aléatoire)\n",
    "                        \n",
    "        Returns:\n",
    "            Texte généré\n",
    "        \"\"\"\n",
    "        # Encoder le texte d'amorce\n",
    "        tokens = self.tokenizer.encode(seed_text)\n",
    "        \n",
    "        # S'assurer que nous avons au moins n-1 tokens\n",
    "        if len(tokens) < self.n - 1:\n",
    "            print(f\"Avertissement: Le texte d'amorce est trop court. Ajout de tokens aléatoires.\")\n",
    "            tokens = [random.randint(0, self.vocab_size - 1) for _ in range(self.n - 1 - len(tokens))] + tokens\n",
    "        \n",
    "        # Ne garder que les n-1 derniers tokens comme contexte initial\n",
    "        context = tuple(tokens[-(self.n - 1):])\n",
    "        \n",
    "        # Générer de nouveaux tokens\n",
    "        generated_tokens = list(tokens)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Calculer les probabilités pour tous les tokens possibles suivant ce contexte\n",
    "            probabilities = [self.get_probability(context, token_id) for token_id in range(self.vocab_size)]\n",
    "            \n",
    "            # Appliquer la température\n",
    "            if temperature != 1.0:\n",
    "                \n",
    "                probabilities = [p ** (1.0 / temperature) for p in probabilities]\n",
    "                # Normaliser pour s'assurer que la somme est égale à 1\n",
    "                sum_probs = sum(probabilities)\n",
    "                if sum_probs > 0:\n",
    "                    probabilities = [p / sum_probs for p in probabilities]\n",
    "            \n",
    "            # Échantillonner le prochain token selon ces probabilités\n",
    "            next_token = random.choices(range(self.vocab_size), weights=probabilities, k=1)[0]\n",
    "            generated_tokens.append(next_token)\n",
    "            \n",
    "            # Mettre à jour le contexte\n",
    "            context = context[1:] + (next_token,)\n",
    "        \n",
    "        # Décoder les tokens générés\n",
    "        generated_text = self.tokenizer.decode(generated_tokens)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ecaea",
   "metadata": {},
   "source": [
    "## Partie 5: Entraînement et génération de texte\n",
    "\n",
    "Maintenant, entraînons notre modèle n-gram et testons la génération de texte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir l'ordre du modèle n-gram\n",
    "n = 3  # Trigramme\n",
    "\n",
    "# Créer et entraîner le modèle\n",
    "ngram_model = NgramLanguageModel(n, tokenizer)\n",
    "\n",
    "# Nous pouvons prendre un sous-ensemble du texte pour accélérer l'entraînement\n",
    "training_text = shakespeare_text  # Premiers 100k caractères\n",
    "print(f\"Entraînement sur {len(training_text)} caractères...\")\n",
    "\n",
    "ngram_model.train(training_text)\n",
    "\n",
    "# Analyser les statistiques du modèle\n",
    "num_contexts = len(ngram_model.context_counts)\n",
    "num_ngrams = len(ngram_model.ngram_counts)\n",
    "print(f\"Nombre de contextes uniques (n-1 grams): {num_contexts}\")\n",
    "print(f\"Nombre de n-grams uniques: {num_ngrams}\")\n",
    "\n",
    "# Contexte le plus fréquent\n",
    "if num_contexts > 0:\n",
    "    most_common_context = max(ngram_model.context_counts.items(), key=lambda x: x[1])\n",
    "    context_tokens = [tokenizer.inverse_vocab[token_id] for token_id in most_common_context[0]]\n",
    "    print(f\"Contexte le plus fréquent: {''.join(context_tokens)} (vu {most_common_context[1]} fois)\")\n",
    "\n",
    "# Générer du texte\n",
    "seed_text = \"Shall I compare thee to a summer's day?\\n\"\n",
    "print(f\"\\nTexte d'amorce: {seed_text}\")\n",
    "\n",
    "# Générer avec différentes températures\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    generated_text = ngram_model.generate_text(seed_text, length=100, temperature=temp)\n",
    "    print(f\"\\nTexte généré (température={temp}):\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram_count in sorted(ngram_model.ngram_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"tokens: {ngram_count[0]}, decoded: {tokenizer.decode(ngram_count[0])}, count: {ngram_count[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa6aea",
   "metadata": {},
   "source": [
    "## Partie 6: Expérimentations\n",
    "\n",
    "Maintenant que nous avons notre infrastructure en place, expérimentons avec différents paramètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6017ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Essayez différentes tailles de vocabulaire pour le tokenizer BPE\n",
    "# 2. Essayez différents ordres de n-grams (n=2, n=3, n=4, etc.)\n",
    "# 3. Essayez différentes températures pour la génération\n",
    "# 4. Comparez les résultats et analysez les différences\n",
    "\n",
    "# Exemple:\n",
    "# Essayons avec un modèle bigram (n=2)\n",
    "bigram_model = NgramLanguageModel(2, tokenizer)\n",
    "bigram_model.train(training_text)\n",
    "print(\"\\nGénération avec un modèle bigram:\")\n",
    "bigram_text = bigram_model.generate_text(seed_text, length=100, temperature=1.0)\n",
    "print(bigram_text)\n",
    "\n",
    "# Essayons avec un modèle 4-gram\n",
    "fourgram_model = NgramLanguageModel(4, tokenizer)\n",
    "fourgram_model.train(training_text)\n",
    "print(\"\\nGénération avec un modèle 4-gram:\")\n",
    "fourgram_text = fourgram_model.generate_text(seed_text, length=100, temperature=1.0)\n",
    "print(fourgram_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2a465",
   "metadata": {},
   "source": [
    "## Partie 7: Questions de réflexion\n",
    "\n",
    "À la fin de ce TP, réfléchissez aux questions suivantes :\n",
    "\n",
    "1. Comment la taille du vocabulaire BPE affecte-t-elle la qualité du texte généré?\n",
    "\n",
    "2. Quelle est l'influence de l'ordre n du modèle n-gram sur:\n",
    "   - La qualité du texte généré\n",
    "   - La capacité du modèle à capturer le style de Shakespeare\n",
    "   - La diversité du texte généré\n",
    "\n",
    "3. Quelles sont les limites principales des modèles n-gram pour la génération de texte?\n",
    "\n",
    "4. Comment pourriez-vous améliorer le modèle n-gram (techniques de lissage, backoff, etc.)?\n",
    "\n",
    "5. Comparez cette approche avec les techniques modernes basées sur les réseaux de neurones. Quels avantages les modèles comme GPT pourraient-ils avoir sur cette approche?\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Dans ce TP, vous avez mis en pratique les concepts de tokenisation BPE et de modèles n-gram vus en cours. Vous avez implémenté ces algorithmes depuis zéro et les avez utilisés pour générer du texte dans le style de Shakespeare.\n",
    "\n",
    "Ces techniques constituent la base historique de la génération de texte, et bien qu'elles aient été largement dépassées par les modèles neuronaux modernes, elles restent importantes pour comprendre les fondements du traitement automatique du langage.\n",
    "\n",
    "Dans le prochain TP, nous explorerons des approches plus modernes basées sur les réseaux de neurones pour la génération de texte. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
